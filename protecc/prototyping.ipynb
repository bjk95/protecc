{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: ucirvine/sms_spam\n",
      "\n",
      "Dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sms', 'label'],\n",
      "        num_rows: 5574\n",
      "    })\n",
      "})\n",
      "\n",
      "Sample data:\n",
      "{'sms': 'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\\n', 'label': 0}\n",
      "\n",
      "Dataset structure after splitting:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sms', 'label'],\n",
      "        num_rows: 4459\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sms', 'label'],\n",
      "        num_rows: 1115\n",
      "    })\n",
      "})\n",
      "\n",
      "Number of labels: 2\n",
      "id2label mapping: {0: 'ham', 1: 'spam'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Load the dataset from Hugging Face Hub\n",
    "dataset_id = \"ucirvine/sms_spam\"\n",
    "print(f\"Loading dataset: {dataset_id}\")\n",
    "raw_datasets = load_dataset(dataset_id)\n",
    "\n",
    "# --- Check dataset structure ---\n",
    "print(\"\\nDataset structure:\")\n",
    "print(raw_datasets)\n",
    "print(\"\\nSample data:\")\n",
    "print(raw_datasets['train'][0])\n",
    "# Expected columns: 'label' (int: 0=ham, 1=spam), 'sms' (string)\n",
    "\n",
    "# --- Split the dataset ---\n",
    "# The dataset only has a 'train' split, so we create train/test splits\n",
    "train_test_split = raw_datasets['train'].train_test_split(test_size=0.2, seed=42, stratify_by_column='label') # 80% train, 20% test\n",
    "\n",
    "# Create a new DatasetDict with 'train' and 'test' splits\n",
    "split_datasets = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'test': train_test_split['test']\n",
    "})\n",
    "print(\"\\nDataset structure after splitting:\")\n",
    "print(split_datasets)\n",
    "\n",
    "# Define label mapping (matches the dataset's 0/1 scheme)\n",
    "num_labels = 2\n",
    "id2label = {0: \"ham\", 1: \"spam\"}\n",
    "label2id = {\"ham\": 0, \"spam\": 1}\n",
    "\n",
    "print(f\"\\nNumber of labels: {num_labels}\")\n",
    "print(f\"id2label mapping: {id2label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing dataset...\n",
      "Sample tokenized data: {'label': tensor(0), 'input_ids': tensor([    35,    544,    498,    614,   1943,   3010,    198, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "# Set padding token if not set\n",
    "if tokenizer.pad_token is None:\n",
    "    if tokenizer.eos_token:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(f\"Set pad_token to eos_token: {tokenizer.pad_token}\")\n",
    "    else:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        print(\"Added new [PAD] token.\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Use the 'sms' column from the dataset\n",
    "    return tokenizer(examples[\"sms\"], padding=\"max_length\", truncation=True, max_length=128) # Increased max_length slightly just in case\n",
    "\n",
    "print(\"\\nTokenizing dataset...\")\n",
    "tokenized_datasets = split_datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove original sms column and any other unnecessary columns\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sms\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "print(\"Sample tokenized data:\", tokenized_datasets['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing dataset...\n",
      "Sample tokenized data: {'label': tensor(0), 'input_ids': tensor([    35,    544,    498,    614,   1943,   3010,    198, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "# Set padding token if not set\n",
    "if tokenizer.pad_token is None:\n",
    "    if tokenizer.eos_token:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(f\"Set pad_token to eos_token: {tokenizer.pad_token}\")\n",
    "    else:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        print(\"Added new [PAD] token.\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Use the 'sms' column from the dataset\n",
    "    return tokenizer(examples[\"sms\"], padding=\"max_length\", truncation=True, max_length=128) # Increased max_length slightly just in case\n",
    "\n",
    "print(\"\\nTokenizing dataset...\")\n",
    "tokenized_datasets = split_datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove original sms column and any other unnecessary columns\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sms\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "print(\"Sample tokenized data:\", tokenized_datasets['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading base model: Qwen/Qwen2-0.5B-Instruct with QLoRA config...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set model pad_token_id to: 151643\n",
      "Applying LoRA adapter to the model...\n",
      "trainable params: 2,164,480 || all params: 496,199,040 || trainable%: 0.4362\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# QLoRA configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nLoading base model: {model_id} with QLoRA config...\")\n",
    "# Load the base model with quantization\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    num_labels=num_labels, # Pass the number of labels (2)\n",
    "    id2label=id2label,     # Pass the mappings\n",
    "    label2id=label2id,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Ensure the model's pad_token_id is set\n",
    "if model.config.pad_token_id is None:\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    print(f\"Set model pad_token_id to: {model.config.pad_token_id}\")\n",
    "\n",
    "# LoRA configuration (verify target_modules if possible, defaults are often okay)\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[ # Common targets, verify for Qwen2 if necessary\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        # \"gate_proj\", \"up_proj\", \"down_proj\" # Less common to include MLP layers\n",
    "    ],\n",
    "     modules_to_save = [\"score\"] # Train the classification head\n",
    ")\n",
    "\n",
    "print(\"Applying LoRA adapter to the model...\")\n",
    "# Create the PEFT model\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_4160/3263657354.py:78: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 4459\n",
      "Effective batch size: 32\n",
      "Steps per epoch: 140\n",
      "Total training steps: 280\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import math # Needed for ceiling division\n",
    "\n",
    "# (Keep the metric definition and compute_metrics function as before)\n",
    "# Define metrics - Including precision, recall, f1 which are important for spam detection\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels, average=\"binary\", pos_label=1)[\"precision\"]\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels, average=\"binary\", pos_label=1)[\"recall\"]\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"binary\", pos_label=1)[\"f1\"]\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "# --- Calculate steps per epoch ---\n",
    "# Define parameters needed for calculation (adjust if you changed them)\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 8\n",
    "GRADIENT_ACCUMULATION_STEPS = 4 # Match the value below\n",
    "NUM_EPOCHS = 2 # Match the value below\n",
    "# Assumes single GPU, adjust if using multiple GPUs (world_size)\n",
    "world_size = 1\n",
    "\n",
    "# Calculate total training steps and steps per epoch\n",
    "num_training_samples = len(tokenized_datasets[\"train\"])\n",
    "effective_batch_size = PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS * world_size\n",
    "steps_per_epoch = math.ceil(num_training_samples / effective_batch_size)\n",
    "max_steps = steps_per_epoch * NUM_EPOCHS # Optional: Can set max_steps instead of num_train_epochs\n",
    "\n",
    "print(f\"Dataset size: {num_training_samples}\")\n",
    "print(f\"Effective batch size: {effective_batch_size}\")\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"Total training steps: {max_steps}\")\n",
    "# ---------------------------------\n",
    "\n",
    "\n",
    "output_dir = \"./models/qwen2-0.5b-sms-spam-ucirvine-qlora\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE, # Make sure this is defined\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=NUM_EPOCHS,           # Make sure this is defined\n",
    "    weight_decay=0.01,\n",
    "    # --- Use Epoch Strategy for Evaluation and Saving ---\n",
    "    # evaluation_strategy=\"epoch\", # Set evaluation strategy to epoch\n",
    "    save_strategy=\"epoch\",       # Set save strategy to epoch (matches eval_strategy)\n",
    "    # eval_steps=... ,           # Not needed for epoch strategy\n",
    "    # save_steps=... ,           # Not needed for epoch strategy\n",
    "    # ----------------------------------------------------\n",
    "    # load_best_model_at_end=True,    # This should now work\n",
    "    metric_for_best_model=\"f1\",     # Optimize for F1-score\n",
    "    logging_strategy=\"steps\",       # Keep logging based on steps (or change to \"epoch\")\n",
    "    logging_steps=50,               # Log every 50 steps (adjust as needed)\n",
    "    fp16=False,\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS, # Make sure this is defined\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Initialize Trainer (remains the same)\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/brad/.cache/pypoetry/virtualenvs/protecc-Ky1pT9Lu-py3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='278' max='278' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [278/278 07:09, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.582800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.080400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.061100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.051000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.043500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brad/.cache/pypoetry/virtualenvs/protecc-Ky1pT9Lu-py3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "Best adapter weights saved to ./qwen2-0.5b-sms-spam-ucirvine-qlora/best_adapter\n",
      "\n",
      "Evaluating best model on test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='140' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [140/140 00:53]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.06340605765581131, 'eval_accuracy': 0.9802690582959641, 'eval_precision': 0.9205298013245033, 'eval_recall': 0.9328859060402684, 'eval_f1': 0.9266666666666666, 'eval_runtime': 54.4333, 'eval_samples_per_second': 20.484, 'eval_steps_per_second': 2.572, 'epoch': 1.989247311827957}\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"\\nStarting training...\")\n",
    "trainer.train()\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# Save the best adapter weights\n",
    "adapter_output_dir = f\"{output_dir}/best_adapter\"\n",
    "trainer.save_model(adapter_output_dir) # Saves only the adapter config and weights\n",
    "\n",
    "# Optional: Save tokenizer\n",
    "tokenizer.save_pretrained(adapter_output_dir)\n",
    "\n",
    "print(f\"Best adapter weights saved to {adapter_output_dir}\")\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "print(\"\\nEvaluating best model on test set...\")\n",
    "eval_results = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])\n",
    "print(\"Evaluation results:\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model for inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PEFT adapter...\n",
      "Model ready for inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "import torch\n",
    "\n",
    "# Load the base model (quantized) and tokenizer\n",
    "base_model_id = \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "adapter_model_id = \"./models/qwen2-0.5b-sms-spam-ucirvine-qlora/best_adapter\"\n",
    "print(\"Loading base model for inference...\")\n",
    "# Need to load with the same quantization config used during training\n",
    "bnb_config_inf = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config=bnb_config_inf,\n",
    "    device_map=\"auto\",\n",
    "    num_labels=num_labels, # Make sure num_labels matches training\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n",
    "\n",
    "# Set padding token if necessary (should match training)\n",
    "if tokenizer.pad_token is None:\n",
    "    if tokenizer.eos_token:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    else:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "if base_model.config.pad_token_id is None:\n",
    "    base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "print(\"Loading PEFT adapter...\")\n",
    "# Load the adapter configuration and merge it with the base model\n",
    "model_with_adapter = PeftModel.from_pretrained(base_model, adapter_model_id)\n",
    "model_with_adapter.eval() # Set the model to evaluation mode\n",
    "\n",
    "print(\"Model ready for inference.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: \n",
      "you need to send me your bank account details to complete the purchase immediately i have images of you\n",
      "\n",
      "Predicted label ID: 0\n",
      "Predicted label: ham\n"
     ]
    }
   ],
   "source": [
    "# Example Inference\n",
    "text = \"\"\"\n",
    "you need to send me your bank account details to complete the purchase immediately i have images of you\n",
    "\"\"\"\n",
    "# text = \"Hey, are we still on for lunch tomorrow?\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(model_with_adapter.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model_with_adapter(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "predicted_label = id2label[predictions] # Use the id2label mapping from training\n",
    "\n",
    "print(f\"Input text: {text}\")\n",
    "print(f\"Predicted label ID: {predictions}\")\n",
    "print(f\"Predicted label: {predicted_label}\") # Should be 'scam' or 'ham'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model (Qwen/Qwen2-0.5B-Instruct) in float16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading adapter from ./qwen2-0.5b-sms-spam-ucirvine-qlora/best_adapter...\n",
      "Merging adapter weights...\n",
      "Adapter merged.\n",
      "Saving merged model and tokenizer to ./qwen2-0.5b-sms-spam-merged...\n",
      "Merged model saved.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# --- Configuration ---\n",
    "base_model_id = \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "adapter_model_id = \"./models/qwen2-0.5b-sms-spam-ucirvine-qlora/best_adapter\" # Path to your saved adapter\n",
    "merged_model_output_path = \"./models/qwen2-0.5b-sms-spam-merged\" # Directory to save the merged model\n",
    "num_labels = 2 # Ensure this matches your fine-tuning task\n",
    "\n",
    "# --- Load Base Model (NOT quantized this time) ---\n",
    "# Load in a standard precision like float16 for merging\n",
    "print(f\"Loading base model ({base_model_id}) in float16...\")\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model_id,\n",
    "    num_labels=num_labels,\n",
    "    torch_dtype=torch.float16, # Use float16 or float32\n",
    "    low_cpu_mem_usage=True, # Helps load large models on less RAM\n",
    "    trust_remote_code=True,\n",
    "    # device_map=\"auto\" # Or load on CPU first: device_map={'': 'cpu'}\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n",
    "\n",
    "# Set padding token if necessary (redundant if already done, but safe)\n",
    "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "if base_model.config.pad_token_id is None: base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# --- Load PEFT Adapter ---\n",
    "print(f\"Loading adapter from {adapter_model_id}...\")\n",
    "# Load the PeftModel directly onto the base model\n",
    "# Ensure the base model is on the correct device (e.g., CPU or GPU) before loading adapter\n",
    "model_with_adapter = PeftModel.from_pretrained(base_model, adapter_model_id)\n",
    "# If base_model was loaded on CPU: model_with_adapter.to('cpu')\n",
    "\n",
    "# --- Merge Adapter and Unload ---\n",
    "print(\"Merging adapter weights...\")\n",
    "merged_model = model_with_adapter.merge_and_unload()\n",
    "print(\"Adapter merged.\")\n",
    "\n",
    "# --- Save the Merged Model ---\n",
    "print(f\"Saving merged model and tokenizer to {merged_model_output_path}...\")\n",
    "merged_model.save_pretrained(merged_model_output_path)\n",
    "tokenizer.save_pretrained(merged_model_output_path)\n",
    "print(\"Merged model saved.\")\n",
    "\n",
    "# Optional: Clean up memory\n",
    "# del base_model, model_with_adapter, merged_model\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scikit-learn version 1.6.1 is not supported. Minimum required version: 0.17. Maximum required version: 1.5.1. Disabling scikit-learn conversion API.\n",
      "Torch version 2.6.0+cu124 has not been tested with coremltools. You may run into unexpected errors. Torch 2.5.0 is the most recent version that has been tested.\n",
      "Failed to load _MLModelProxy: No module named 'coremltools.libcoremlpython'\n",
      "Failed to load _MLCPUComputeDeviceProxy: No module named 'coremltools.libcoremlpython'\n",
      "Failed to load _MLGPUComputeDeviceProxy: No module named 'coremltools.libcoremlpython'\n",
      "Failed to load _MLNeuralEngineComputeDeviceProxy: No module named 'coremltools.libcoremlpython'\n",
      "Failed to load _MLModelProxy: No module named 'coremltools.libcoremlpython'\n",
      "Failed to load _MLComputePlanProxy: No module named 'coremltools.libcoremlpython'\n",
      "Failed to load _MLModelProxy: No module named 'coremltools.libcoremlpython'\n",
      "Failed to load _MLModelAssetProxy: No module named 'coremltools.libcoremlpython'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./qwen2-0.5b-sms-spam-merged for Core ML conversion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing the PyTorch model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brad/.cache/pypoetry/virtualenvs/protecc-Ky1pT9Lu-py3.10/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:285: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  elif sliding_window is None or key_value_length < sliding_window:\n",
      "/home/brad/.cache/pypoetry/virtualenvs/protecc-Ky1pT9Lu-py3.10/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:719: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.shape[-1] > target_length:\n",
      "/home/brad/.cache/pypoetry/virtualenvs/protecc-Ky1pT9Lu-py3.10/lib/python3.10/site-packages/transformers/integrations/sdpa_attention.py:47: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  is_causal = query.shape[2] > 1 and causal_mask is None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during tracing: Tracer cannot infer type of SequenceClassifierOutputWithPast(loss=None, logits=tensor([[ 1.9775, -0.8584]], dtype=torch.float16, grad_fn=<IndexBackward0>), past_key_values=<transformers.cache_utils.DynamicCache object at 0x7f0d367eaaa0>, hidden_states=None, attentions=None)\n",
      ":Only tensors and (possibly nested) tuples of tensors, lists, or dicts are supported as inputs or outputs of traced functions, but instead got value of type DynamicCache.\n",
      "Attempting direct conversion (may have limitations)...\n",
      "Direct conversion also failed: Model must either be a TorchScript object (or .pt or .pth file) or an ExportedProgram object (if using torch.export based API), received: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2ForSequenceClassification'>\n"
     ]
    }
   ],
   "source": [
    "import coremltools as ct\n",
    "import torch\n",
    "import transformers # Ensure transformers is imported\n",
    "\n",
    "# --- Load your MERGED and potentially QUANTIZED PyTorch model ---\n",
    "# If you applied PyTorch quantization, load that model.\n",
    "# If aiming for quantization during conversion, load the merged FP16 model.\n",
    "model_path = \"./models/qwen2-0.5b-sms-spam-merged\" # Path to the merged FP16 model\n",
    "output_coreml_path = \"./models/qwen2-0.5b-sms-spam.mlpackage\"\n",
    "\n",
    "print(f\"Loading model from {model_path} for Core ML conversion...\")\n",
    "# It's often best to explicitly load the model class\n",
    "pytorch_model = transformers.AutoModelForSequenceClassification.from_pretrained(model_path, torch_dtype=torch.float16)\n",
    "pytorch_model.eval() # Set to evaluation mode\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# --- Prepare Example Input for Tracing ---\n",
    "# Core ML conversion often uses tracing, requiring example inputs.\n",
    "# The shape should match what the model expects during inference.\n",
    "example_text = \"This is a sample input text for tracing.\"\n",
    "# max_length should match tokenizer settings used during training/inference\n",
    "tokenized_input = tokenizer(example_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Extract tensors required by the model's forward pass\n",
    "# Typically 'input_ids' and 'attention_mask'\n",
    "example_input_ids = tokenized_input['input_ids']\n",
    "example_attention_mask = tokenized_input['attention_mask']\n",
    "# Put example inputs on CPU for tracing usually\n",
    "example_inputs_tuple = (example_input_ids.cpu(), example_attention_mask.cpu())\n",
    "# Or as a dictionary if the converter prefers that\n",
    "# example_inputs_dict = {\"input_ids\": example_input_ids.cpu(), \"attention_mask\": example_attention_mask.cpu()}\n",
    "\n",
    "\n",
    "# --- Trace the PyTorch model ---\n",
    "print(\"Tracing the PyTorch model...\")\n",
    "try:\n",
    "    # Use torch.jit.trace with the model's forward method inputs\n",
    "    traced_model = torch.jit.trace(pytorch_model, example_inputs_tuple, strict=False)\n",
    "    print(\"Model traced successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during tracing: {e}\")\n",
    "    # Tracing can be tricky; alternative might be script conversion or specific exporters\n",
    "    # For Hugging Face models, sometimes direct conversion works better\n",
    "    traced_model = None # Indicate failure\n",
    "\n",
    "# --- Convert using coremltools ---\n",
    "# This is a simplified example. You'll need to configure inputs/outputs\n",
    "# and crucially, quantization options.\n",
    "if traced_model: # Proceed if tracing worked\n",
    "    print(\"Converting traced model to Core ML...\")\n",
    "\n",
    "    # Define input types (match the traced inputs)\n",
    "    input_ids = ct.TensorType(name=\"input_ids\", shape=example_input_ids.shape, dtype=np.int32)\n",
    "    attention_mask = ct.TensorType(name=\"attention_mask\", shape=example_attention_mask.shape, dtype=np.int32)\n",
    "\n",
    "    # --- Quantization during Conversion (Example: FP16 weights) ---\n",
    "    # For INT8 or other types, explore ct.optimize.coreml.* or specific compute_precision options\n",
    "    # Example: weights quantized to FP16\n",
    "    quantization_options = ct.OptimizeDevice.QuantizationOptions(\n",
    "        global_quantization_type='linear', # or 'kmeans' etc.\n",
    "        quantization_mode='weight_symmetric',\n",
    "        weight_dtype=ct.precision.FLOAT16 # Example: FP16 weights. For INT8, explore options.\n",
    "    )\n",
    "    # Apply optimization profile targeting ANE if possible\n",
    "    optimization_profile = ct.OptimizeDevice(quantization_options=quantization_options, allow_low_precision_weights=True)\n",
    "\n",
    "\n",
    "    # Convert the model\n",
    "    # minimum_deployment_target is important for compatibility and features\n",
    "    mlmodel = ct.convert(\n",
    "        traced_model,\n",
    "        inputs=[input_ids, attention_mask],\n",
    "        # outputs=[ct.TensorType(name=\"logits\")], # Define output name if needed\n",
    "        convert_to=\"mlprogram\", # Recommended modern format\n",
    "        minimum_deployment_target=ct.target.iOS16, # Or newer, e.g., iOS17\n",
    "        compute_units=ct.ComputeUnit.ALL, # Allow CPU, GPU, ANE\n",
    "        # optimization_profile=optimization_profile, # Apply quantization/optimization profile\n",
    "        # Pass compute_precision=ct.precision.FLOAT16 or other options here if applicable\n",
    "    )\n",
    "\n",
    "    print(f\"Saving Core ML model to {output_coreml_path}...\")\n",
    "    mlmodel.save(output_coreml_path)\n",
    "    print(\"Core ML model saved.\")\n",
    "\n",
    "else: # Alternative if tracing failed (might work better for HF models)\n",
    "    print(\"Attempting direct conversion (may have limitations)...\")\n",
    "    try:\n",
    "        mlmodel = ct.convert(\n",
    "            pytorch_model, # Pass the original HF model\n",
    "            source=\"pytorch\",\n",
    "            inputs=[\n",
    "                ct.TensorType(name=\"input_ids\", shape=(1, ct.RangeDim(1, 128)), dtype=np.int32), # Use dynamic shape\n",
    "                ct.TensorType(name=\"attention_mask\", shape=(1, ct.RangeDim(1, 128)), dtype=np.int32)\n",
    "            ],\n",
    "            convert_to=\"mlprogram\",\n",
    "            minimum_deployment_target=ct.target.iOS16,\n",
    "            compute_units=ct.ComputeUnit.ALL,\n",
    "            # Add quantization/optimization options here as well\n",
    "        )\n",
    "        print(f\"Saving Core ML model to {output_coreml_path}...\")\n",
    "        mlmodel.save(output_coreml_path)\n",
    "        print(\"Core ML model saved.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Direct conversion also failed: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protecc-Ky1pT9Lu-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
